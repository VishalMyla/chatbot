# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C9JKZCSxPIxS_H3QwGEh1k-bzw2dODZQ
"""

# !pip install pinecone-client requests langchain huggingface_hub sentence_transformers

# !pip install pinecone

# !pip install -U langchain-community

import os
import pinecone
import requests
from langchain.embeddings import HuggingFaceEmbeddings

# Step 1: Initialize the HuggingFace embedding model
def initialize_embeddings():
    """
    Initialize the embedding model.
    """
    print("Initializing embedding model...")
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",  # A good lightweight model for embeddings
        model_kwargs={'device': 'cpu'}
    )
    return embeddings

# Initialize embeddings
embeddings = initialize_embeddings()

# Step 2: Initialize Pinecone
from pinecone import Pinecone, ServerlessSpec

# Create an instance of Pinecone with your API key and serverless spec
pc = Pinecone(
    api_key="pcsk_itU7o_TTNhaiqXCBUAjedBCYTtXxGB2v1DCh9GRb2uSRVkj8eDQRwjEtPQdfUPFUgm2to",
    spec=ServerlessSpec(cloud="aws", region="us-east-1")
)

# Connect to your index (replace "etq1" with your actual index name)
index = pc.Index("etq1")

def get_hf_embedding(text):
    """
    Use HuggingFaceEmbeddings to generate an embedding for the input text.
    """
    return embeddings.embed_query(text)  # Using embed_query() from the HuggingFaceEmbeddings API

def call_groq_api(messages):
    """
    Call the Groq API with a chat messages and return the response.
    """
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": "Bearer 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",
        "Content-Type": "application/json"
    }
    data = {
        "model": "meta-llama/llama-4-scout-17b-16e-instruct", # or another groq supported model.
        "messages": messages
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        json_resp = response.json()
        return json_resp["choices"][0]["message"]["content"]
    else:
        raise Exception(f"Groq API request failed: {response.status_code}, {response.text}")

#Example usage.
messages = [{"role": "user", "content": "Explain the importance of fast language models"}]
result = call_groq_api(messages)
print(result)

def process_query(user_query):
    """
    For a given user query, generate an embedding, retrieve similar vectors from Pinecone,
    extract their textual content, build a prompt, and call the LLM API to generate an answer.
    """
    # Generate embedding for the user query
    query_embedding = get_hf_embedding(user_query)

    # Query Pinecone for the most similar vectors
    query_results = index.query(vector=query_embedding, top_k=3, include_metadata=True)

    # Extract text from metadata of each matching vector (assumes each record's metadata has a "text" field)
    context_texts = [match["metadata"].get("text", "") for match in query_results.get("matches", [])]
    context = "\n".join(context_texts)

    # Construct a prompt for the LLM (LLM expects text, not raw embeddings)
    prompt = (
        f"Context:\n{context}\n\n"
        f"User Query: {user_query}\n\n"
        f"Provide a clear and comprehensive answer based on the above context:"
    )

    # Call the Groq API with the prompt and return its response
    return call_groq_api([{"role": "user", "content": prompt}])

# if __name__ == "__main__":
#     user_input = input("Enter your query: ")
#     try:
#         answer = process_query(user_input)
#         print("\nLLM Response:")
#         print(answer)
#     except Exception as e:
#         print("Error processing query:", e)

from flask import Flask, request
from flask_cors import CORS  # Import CORS

app = Flask(__name__)
CORS(app)    # Enable CORS for all routes

@app.route('/process', methods=['POST'])
def process():
    try:
        data = request.get_data(as_text=True)  # raw string
        print(data)
        answer = process_query(data)  # Assuming you have a process_query function
        print("\nLLM Response:")
        return answer, 200
    except Exception as e:
        err = "Error processing query:" + str(e) # important to change e to string.
    return err, 400

app.run(debug=True, host='127.0.0.1', port=5000)

